import streamlit as st
from openai import OpenAI

# Show title and description.
st.title("ğŸ’¬ SafeHarbor AI ")
st.write(
    "Welcome to **SafeHarbor AI**, your trusted companion in the fight against financial fraud and investment scams! In today's fast-paced digital world, staying vigilant against deceptive schemes is more important than ever. FraudShield AI leverages cutting-edge artificial intelligence to empower individuals with the knowledge and tools to protect their finances and make informed decisions."
    """
    \n **What Does FraudShield AI Do?**
- **Financial Fraud Detection:** The chatbot helps you identify red flags in financial offers, investment opportunities, and transactions. Whether it's phishing, Ponzi schemes, or cryptocurrency scams, FraudShield AI is here to guide you.
- **Education on Financial Safety:** Learn essential skills to safeguard your money, understand common fraud tactics, and build financial literacy to make smarter decisions.
- **Real-Time Consultation:** Have doubts about a suspicious email, call, or investment pitch? FraudShield AI provides instant advice and actionable insights.
- **Scam Awareness Updates:** Stay up-to-date with the latest trends in financial fraud and scams, so you're always prepared.

 **Why Choose FraudShield AI?**
- **Accessible and Reliable:** Available 24/7, FraudShield AI is your go-to resource for combating financial fraud and gaining confidence in managing your finances.
- **Empowering You:** By educating and assisting you, the chatbot ensures you're not only protected but also equipped to prevent fraud on your own.

Join the movement to protect yourself and others from financial fraud. With FraudShield AI, you're in control of your financial future. Let's build a safer and smarter financial world together!
"""
)






# Ask user for their OpenAI API key via `st.text_input`.
# Alternatively, you can store the API key in `./.streamlit/secrets.toml` and access it
# via `st.secrets`, see https://docs.streamlit.io/develop/concepts/connections/secrets-management
openai_api_key = st.text_input("OpenAI API Key", type="password")
if not openai_api_key:
    st.info("Please add your OpenAI API key to continue.", icon="ğŸ—ï¸")
else:

    # Create an OpenAI client.
    #client = OpenAI(api_key=openai_api_key)
    client = OpenAI(
        base_url="wss://spark-api.xf-yun.com/v1.1/chat",
        api_key="46909103&MTU1YWQ5MGFkNzhkYTBmM2NjNzMyYTBj&e8583878e386f949c0776808a7160823", 
    )


    # Create a session state variable to store the chat messages. This ensures that the
    # messages persist across reruns.
    if "messages" not in st.session_state:
        st.session_state.messages = []

    # Display the existing chat messages via `st.chat_message`.
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Create a chat input field to allow the user to enter a message. This will display
    # automatically at the bottom of the page.
    if prompt := st.chat_input("What is up?"):

        # Store and display the current prompt.
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        # Generate a response using the OpenAI API.
        stream = client.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": m["role"], "content": m["content"]}
                for m in st.session_state.messages
            ],
            stream=True,
        )



completion = client.chat.completions.create(
    model="generalv3.5",  # e.g. gpt-35-instant
    max_tokens=6000,
    messages=[
        {
            "role": "system",
            "content":  "ä½ æ˜¯ä¸€ä¸ªéå¸¸æ£’çš„Wrapperä»£ç ç”ŸæˆAgentï¼Œé€šè¿‡è·Ÿç”¨æˆ·ä¸æ–­å¯¹è¯ï¼Œç†è§£ç”¨æˆ·éœ€æ±‚å¹¶æŒ‰ç…§æ—¢å®šèŒƒå¼å®Œæˆæ’ä»¶wrapperä»£ç ç¼–å†™ã€‚",
            "role": "user",
            "content":"\nWrapperä»£ç ç”¨äºæŠŠå¦‚æ¨¡å‹æ¨ç†ï¼Œä¸‰æ–¹API åŒ…è£…æˆç¬¦åˆASEåè®®è§„èŒƒçš„çš„æœåŠ¡ã€‚\n\nWrapperçš„æ—¢å®šèŒƒå¼å¦‚ä¸‹:\n```\n### start ä»¥ä¸‹æ˜¯å¯¼å…¥å¿…è¦çš„å·¥å…·åŒ…ä»£ç åŒºå—\nimport json\nimport os.path\n\nfrom aiges.core.types import *\n\ntry:\n    from aiges_embed import ResponseData, Response, DataListNode, DataListCls, SessionCreateResponse  # c++\nexcept:\n    from aiges.dto import Response, ResponseData, DataListNode, DataListCls, SessionCreateResponse\n\nfrom aiges.sdk import WrapperBase, \\\n    ImageBodyField, \\\n    StringBodyField, StringParamField\nfrom aiges.utils.log import log, getFileLogger\n### end ä»¥ä¸‹æ˜¯å¯¼å…¥å¿…è¦çš„å·¥å…·åŒ…ä»£ç åŒºå—\n\n### start ä¸‹é¢æ˜¯å¼•ç”¨ä½ çš„æ¨ç†ä»£ç åŒºå—\nfrom  inference import Engine\n\n### end\n\n### start ä¸‹é¢æ˜¯è®¾ç½®æœåŠ¡çš„è¯·æ±‚å’Œå“åº”å­—æ®µä»£ç åŒºå—\n# å®šä¹‰æœåŠ¡çš„è¯·æ±‚å‚æ•°\nclass UserRequest(object):\n    input1 = ImageBodyField(key=\"img\", path=\"test_data/0.png\") # ä»£è¡¨æœåŠ¡éœ€è¦è¾“å…¥ä¸€ä¸ª keyä¸º imgçš„å›¾ç‰‡å­—æ®µï¼Œè¯¥å­—æ®µéœ€è¦ä¼ é€’å›¾ç‰‡çš„äºŒè¿›åˆ¶bytes. input1ä¸ºå›ºå®šæ ¼å¼å‘½åï¼Œå¦‚æœæœ‰å¤šä¸ªè¾“å…¥ï¼Œåˆ™ä»¥input1, input2,input3... åˆ†åˆ«å‘½å\n\n\n# å®šä¹‰æ¨¡å‹çš„è¾“å‡ºå‚æ•°\nclass UserResponse(object):\n    accept1 = StringBodyField(key=\"result\") # ä»£è¡¨æœåŠ¡éœ€è¦è¾“å‡ºä¸€ä¸ª keyä¸º resultï¼Œè¯¥å­—æ®µéœ€ç±»å‹æ˜¯ä¸€ä¸ªStringç±»å‹. accept1ï¼Œå¦‚æœæœ‰å¤šä¸ªè¾“å‡ºï¼Œåˆ™ä»¥accept1, accept2, accept3... åˆ†åˆ«å‘½å\n\n### end\n\n\n\n### ä¸‹é¢ä»¥mnistæ‰‹å†™ä½“æ¨¡å‹æ¨ç†æ’ä»¶ä¸ºä¾‹\n\n### start æ¨ç†çš„æ ¸å¿ƒåŒºå—æ˜¯å®ç°å¦‚ä¸‹Wrapperç±»ä»¥åŠå…¶æ–¹æ³•ï¼Œ\n# wrapperç±»å¿…é¡»ç»§æ‰¿WrapperBase\nclass Wrapper(WrapperBase):\n    serviceId = \"mnist\" # æœåŠ¡è‹±æ–‡åï¼Œç”±ç”¨æˆ·è¾“å…¥ï¼Œé»˜è®¤å¯ä»¥ä¸º default\n    version = \"v1\" # ç‰ˆæœ¬å·å›ºå®šä¸ºv1\n    call_type = 1 # è°ƒç”¨ç±»å‹å›ºå®šä¸º1\n    requestCls = UserRequest() # è¿™é‡Œå¼•ç”¨äº†ä¸Šè¿°å®šä¹‰çš„ UserRequestç±»å¹¶å®ä¾‹åŒ–ä¸ºrequestCls\n    responseCls = UserResponse() # è¿™é‡Œå¼•ç”¨äº†ä¸Šè¿°å®šä¹‰çš„UserResponseå¹¶å®ä¾‹åŒ–ä¸ºresponseCls\n    model = None\n\n    def __init__(self, *args, **kwargs):\n        super().__init__(*args, **kwargs)\n        self.transform = None\n        self.device = None\n        self.filelogger = None\n\n    def wrapperInit(self, config: {}) -> int:\n        log.info(\"Initializing ...\")\n        self.filelogger = getFileLogger()\n        self.engine = Engine(config) # å®ä¾‹åŒ–ç”¨æˆ·å®ç°çš„inference.pyä¸­çš„ Engineç±»\n        return 0\n\n    def wrapperLoadRes(self, reqData: DataListCls, resId: int) -> int:\n        # è¯¥æ–¹æ³•ç”¨äºåŠ è½½ä¸ªæ€§åŒ–èµ„æºï¼Œé»˜è®¤æ— éœ€å®ç°\n        return 0\n\n    def wrapperUnloadRes(self, resId: int) -> int:\n        # è¯¥æ–¹æ³•ç”¨äºå¸è½½ä¸ªæ€§åŒ–èµ„æºï¼Œé»˜è®¤æ— éœ€å®ç°\n        return 0\n\n    def wrapperOnceExec(self, params: {}, reqData: DataListCls, usrTag: str = \"\", persId: int = 0) -> Response:\n        # éæµå¼æ¨ç†æ¥å£ï¼Œä¼šæŠŠreqDataæ•°æ®è½¬æ¢é€å…¥åˆ° engineçš„ inferæ–¹æ³•ä¸­\n        # ä½¿ç”¨Responseå°è£…result\n        res = Response()\n        ctrl = params.get(\"ctrl\", \"default\")\n        self.filelogger.info(\"got reqdata , %s\" % reqData.list)\n        imagebytes = reqData.get(\"img\").data\n        img = Image.open(io.BytesIO(imagebytes)) # å¯¹äºå›¾ç‰‡æ•°æ®ï¼Œéœ€è¦ä½¿ç”¨Image.openè½¬æˆå†…å­˜bytesæµ\n        try:\n            result = self.engine.infer(img) # è¯¥æ–¹æ³•éœ€è¦æ ¹æ®self.engine.inferæ–¹æ³•è¿”å›ä¿®æ”¹\n            log.info(\"infer result ###:%d\" % int(result))\n            # å¦‚ä¸‹ç»“æ„ç”¨ç”¨æˆ·å®šä¹‰ï¼Œè¿™é‡ŒmnistæœåŠ¡è¿”å›ä¸€ä¸ª jsonï¼ŒåŒ…å« result å’Œmsg\n            text_json = {\n                \"result\": int(result),\n                \"msg\": \"result is: %d\" % int(result)\n            }\n            accept1Data = ResponseData() # responseCls ä¸­åªæœ‰ä¸€ä¸ªæ•°æ®æ®µaccept1ï¼Œæ‰€ä»¥åªéœ€è¦å®ä¾‹åŒ–ä¸€ä¸ªResponseData\n            accept1Data.key = \"result\" # ç”±äºä¸Šè¿°å“åº”ç±»responseCls è®¾ç½®çš„keyä¸º result\n            accept1Data.setDataType(DataText) # å“åº”æ˜¯æ˜¯StringBody éœ€è¦è®¾ç½®ä¸º DataText\n            accept1Data.status = Once # éæµå¼è®¾ç½®ä¸ºOnce\n            accept1Data.setData(json.dumps(text_json).encode(\"utf-8\")) # æŠŠtext_jsonè½¬æ¢ä¸º str æ”¾å…¥å“åº”æ•°æ®ä¸­\n            res.list = [accept1Data] # å°†accept1Data æ”¾å…¥reså“åº”ç±»ä¸­\n        except Exception as e:\n            log.error(e)\n            # é”™è¯¯é€»è¾‘å¤„ç†\n            return res.response_err(100)\n        return res\n\n    def wrapperFini(cls) -> int:\n        return 0\n\n    def wrapperError(cls, ret: int) -> str:\n        if ret == 100:\n            return \"wrapper exec exception here...\"\n        return \"\"\n    def wrapperCreate(cls, params: {}, sid: str, persId: int = 0) -> SessionCreateResponse:\n        print(params)\n        i = random.randint(1,30000)\n        print(sid)\n        return f\"hd-test-{i}\"\n    '''\n        æ­¤å‡½æ•°ä¿ç•™æµ‹è¯•ç”¨ï¼Œä¸å¯åˆ é™¤\n    '''\n\n    def wrapperTestFunc(cls, data: [], respData: []):\n        pass\n\n### end\n\n\n```\n\nç°åœ¨Wrapperçš„å¼•å…¥çš„inference.pyä»£ç å¦‚ä¸‹:\n\n```python\nimport os\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n        self.dropout1 = nn.Dropout(0.25)\n        self.dropout2 = nn.Dropout(0.5)\n        self.fc1 = nn.Linear(9216, 128)\n        self.fc2 = nn.Linear(128, 10)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = F.relu(x)\n        x = self.conv2(x)\n        x = F.relu(x)\n        x = F.max_pool2d(x, 2)\n        x = self.dropout1(x)\n        x = torch.flatten(x, 1)\n        x = self.fc1(x)\n        x = F.relu(x)\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        output = F.log_softmax(x, dim=1)\n        return output\n\n## æ¨ç†å¼•æ“ç±»,ç”±ç”¨æˆ·å®ç°\nclass Engine():\n    def __init__(self,config):\n        self.config = config\n        self.device = \"cpu\"\n        self.model = Net().to(self.device)\n        ptfile = os.path.join(os.path.dirname(__file__), \"train\", \"mnist_cnn.pt\")\n        self.model.load_state_dict(torch.load(ptfile))  # æ ¹æ®æ¨¡å‹ç»“æ„ï¼Œè°ƒç”¨å­˜å‚¨çš„æ¨¡å‹å‚æ•°\n        self.model.eval()\n        self.transform = transforms.Compose([\n            transforms.Grayscale(num_output_channels=1),\n            transforms.Resize([28, 28]),\n            transforms.ToTensor(),\n            transforms.Normalize((0.1307,), (0.3081,))\n        ])\n\n    def infer(self, config, data):\n        img = self.transform(data).unsqueeze(0)\n        img.to(self.device)\n        result = self.model(img).argmax()\n        return result\n\n```\n\n\nè¯·æ ¹æ®ä¸Šè¿°è§„èŒƒä»£ç ä¸­æ³¨é‡Šéƒ¨åˆ†å’Œå¦‚ä¸‹ç”¨æˆ·çš„éœ€æ±‚è°ƒæ•´å¹¶ç”Ÿæˆæ–°çš„wrapper.pyï¼Œæ³¨æ„inference.py ä¸éœ€è¦åˆå…¥wrapper.py.ä»£ç å¿…é¡»ç¬¦åˆpythonè§„èŒƒ\n\nç”¨æˆ·éœ€æ±‚å¦‚ä¸‹:\nå¸®æˆ‘æ ¹æ®inference.pyä¸­çš„æ¨ç†å®ç°è°ƒæ•´ä¸‹ wrapper.pyä¸­è°ƒç”¨ inferæ–¹æ³•éƒ¨åˆ†",
        },
    ],

)
print(completion.model_dump()["choices"][0]["message"]["content"])

        # Stream the response to the chat using `st.write_stream`, then store it in 
        # session state.
        with st.chat_message("assistant"):
            response = st.write_stream(stream)
        st.session_state.messages.append({"role": "assistant", "content": response})

